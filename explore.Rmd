---
title: "Data Science Capstone - Exploratory Data Analysis"
author: "patmau"
date: "Dec 29, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(quanteda)
library(ngram)
quanteda_options(threads = 2)

```

## Introduction

The ultimate goal in this capstone project is to create a shiny app for the predictive modeling of English text. The raw data to create the model was provided by the company [SwiftKey](http://swiftkey.com), which was purchased by Microsoft in 2016. The raw data contains text collected from blog posts, tweets, and news articles, and was downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

In this report, we describe a first exploratory analysis of the raw data. The data contains text in English, German, Russian, and Finnish, but we will restrict ourselves to the English language in this project.


## Data size

Text is provided in three text files corresponding to the sources of the text (Twitter, blogs, news). Each line in a file corresponds to an entry in the respective source (e.g., a tweet or a blog post).
In a first step, we look at the size of the data. The table below lists file sizes, memory used to store the data in memory, the number of entries in each file and the number of characters.

```{r read files}

# read files and compute file sizes and number of entries/characters

#srcDir <- "data/final/en_US"
srcDir <- "data/sample10"
src <- c(blogs = "blogs", news = "news", twitter = "twitter")

files <- sapply(src, function(s) {
#      fileName <- paste("en_US", s, "txt", sep = ".")
    fileName <- paste("sample10.en_US", s, "txt", sep = ".")
    paste(srcDir, fileName, sep = "/")
})

fileSizes <- data.frame("File size [MB]" = sapply(src, function(s) {
  file.size(files[s]) / 1024 ^ 2
}), check.names = FALSE)

lines <- sapply(src, function(s) {
    con <- file(files[s], "r")
    lns <- readLines(con, skipNul = TRUE)
    close(con)
    lns
})

counts <- data.frame(
    sapply(lines, function(lns) {
        size <- object.size(lns) / 1024^2
        lineCount <- length(lns)
        c("Data size [MB] in memory" = size, "Entries" = lineCount)
    }) 
)

charCounts <- sapply(lines, function(lns) {
    sapply(lns, nchar, USE.NAMES = FALSE)
})

totalChars <- data.frame(Characters = sapply(charCounts, sum))

counts <- rbind(t(fileSizes), counts, t(totalChars))

counts$total <- rowSums(counts)
kable(counts, digits = 0, format.args = list(big.mark = ' '),
      caption = "Data size and number of entries and characters in source files")

```

All three source files contain a similar total amount of data, although the Twitter file contains roughly twice as many entries as the other two files (tweets tend to be shorter than blog posts or news articles, see below).
The total amount of data is `r round(counts$total[2])` MB in memory. 


## Sample the data

The complete raw data set is rather large, and it turns out that the memory requirement for tokenization of the complete data set exceeds what is available on the laptop used for this analysis (8 GB).
Therefore, further analysis will be performed on a subset constructed by randomly sampling 10% of all entries from each source file. 

The figure below compares the distributions of the length of entries (number of characters) in the various sources. Solid lines represent the complete data set, dashed lines represent sampled data.

```{r sample}
# sample raw data

set.seed(2020)
samples <- sapply(lines, function(lns) {
    sample(lns, length(lns) / 10, replace = FALSE)
})

sampleText <- rbind(
    data.frame(src = "blogs", txt = samples$blogs),
    data.frame(src = "news", txt = samples$news),
    data.frame(src = "twitter", txt = samples$twitter)
)

sampleText$charCount <- nchar(sampleText$txt)

# get rid of large objects
rm(list = c("lines", "samples"))

```

```{r graphs, message = FALSE}
# character density plots

# convert to long format for ggplot
meltedChar <- melt(charCounts, value.name = "characters") %>%
    filter(characters <= 600)

meltedSample <- melt(sampleText, id.vars = "src", measure.vars = "charCount", value.name = "characters") %>%
    filter(characters <= 600)

ggplot(meltedChar) +
    geom_density(aes(x = characters, col = L1)) +
    geom_density(data = meltedSample, aes(x = characters, col = src), linetype = 2) +
    scale_color_discrete(name = "Source") +
    ggtitle("Density plot of characters per entry")
  
```

It is obvious from the plot that Twitter was sourced at a time when tweets were still limited to 140 characters. Blog posts and news articles show a broader distribution of the length of entries. 
The plot also shows that the data sample adequately represents the full set in terms of the length of entries (which could be used as a crude measure of complexity of a text). 

## Words

We use the [quanteda package](http://quanteda.io) to tokenize text. For an initial analysis, we restrict ourselves to words, and remove punctuation characters, symbols, numbers, URLs, etc., from the text. Also, all text is converted to lower case.
Tokenization is performed once including [stop words](https://en.wikipedia.org/wiki/Stop_word), and a second time excluding all stop words.

```{r tokenize, message=FALSE, eval=TRUE}
# tokenize 

tokens_all <- tokens(sampleText$txt,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE)

tokens_nosw <- tokens_remove(tokens_all, stopwords("english"))
```

```{r frequencyplot, eval=TRUE}
# functions for computing/plotting token frequencies

token_freq <- function(tokens) {
  dfm <- dfm(tokens)
  freq <- textstat_frequency(dfm)
  freq$cumulFreq <- cumsum(freq$frequency)
  freq
}

freqPlot <- function(tk_freq, ntop, xlab = "n-gram", ylab = "Count", title = NULL) {
    tf <- tk_freq[1:ntop, ]
    ggplot(tf, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_col() +
        xlab(xlab) + ylab(ylab) +
        labs(title = title) +
        coord_flip()
}

```

```{r unigrams, eval=TRUE}
# unigrams

tkfreq.uni <- token_freq(tokens_all)
uniplot.fr <- freqPlot(tkfreq.uni, 20, xlab = "Word (unigram)", title = "Including stopwords")

tkfreq.uni.nosw <- token_freq(tokens_nosw)
noswplot.fr <- freqPlot(tkfreq.uni.nosw, 20, xlab = "Word (unigram)", title = "Excluding stopwords")

totalTokens <- tail(tkfreq.uni$cumulFreq, 1)

nTokens <- dim(tkfreq.uni)[1]

stopwordCumulFreq <- sum(tkfreq.uni$frequency[tkfreq.uni$feature %in% stopwords("english")])
nOccurOnce <- sum(tkfreq.uni$frequency == 1)

minIdx_nonStopword <- min(which(!(tkfreq.uni$feature %in% stopwords("english"))))
```

The corpus contains a total of `r format(totalTokens, big.mark=" ")` words, composed from `r format(nTokens, big.mark=" ")` unique words.
From these unique words, `r format(nOccurOnce, big.mark=" ")` (`r round(100.0 * nOccurOnce/nTokens)`%) appear only once.
They account for `r round(100.0 * nOccurOnce / totalTokens)`% of all words in the corpus.


```{r unigramPlots, eval=TRUE}
grid.arrange(uniplot.fr, noswplot.fr, ncol = 2, top = "Most Frequent Words")

ggplot(tkfreq.uni) +
  geom_line(aes(x = 1:length(cumulFreq), y = cumulFreq)) +
  geom_hline(yintercept = (0.9 * tail(tkfreq.uni$cumulFreq, 1)), col = "blue", linetype = "dashed") +
  ylab("Cumulative Frequency") + xlab("Token index (from most to least occurring)")
```

The first non-stop word is "`r tkfreq.uni$feature[minIdx_nonStopword]`" at position `r minIdx_nonStopword`.
Stopwords account for `r round(100.0 * stopwordCumulFreq / totalTokens)`% of all tokens.


```{r bigrams, eval=FALSE}

tk.bi <- tokens_ngrams(tokens_all, n = 2, concatenator = " ")
tkfreq.bi <- token_freq(tk.bi)
biplot <- freqPlot(tkfreq.bi, 20, xlab = "Bigram", title = "Including stopwords")
rm(list = "tk.bi")

tk.nosw.bi <- tokens_ngrams(tokens_nosw, n = 2, concatenator = " ")
tkfreq.nosw.bi <- token_freq(tk.nosw.bi)
noswplot <- freqPlot(tkfreq.nosw.bi, 20, xlab = "Bigram", title = "Excluding stopwords")
rm(list = "tk.nosw.bi")

grid.arrange(biplot, noswplot, ncol = 2)

ggplot(tkfreq.bi) +
  geom_line(aes(x = 1:length(cumulFreq), y = cumulFreq)) +
  ylab("Cumulative Frequency") + xlab("2-gram index (from most to least occurring)")

```

```{r trigrams, eval=FALSE}

tk.tri <- tokens_ngrams(tokens_all, n = 3, concatenator = " ")
tkfreq.tri <- token_freq(tk.tri)
triplot <- freqPlot(tkfreq.tri, 20, xlab = "Trigram", title = "Including stopwords")
rm(list = "tk.tri")

tk.nosw.tri <- tokens_ngrams(tokens_nosw, n = 3, concatenator = " ")
tkfreq.nosw.tri <- token_freq(tk.nosw.tri)
noswplot <- freqPlot(tkfreq.nosw.tri, 20, xlab = "Trigram", title = "Excluding stopwords")
rm(list = "tk.nosw.tri")

grid.arrange(triplot, noswplot, ncol = 2)

```
