---
title: "Data Science Capstone - Exploratory Data Analysis"
author: "patmau"
date: "Dec 29, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(quanteda)
quanteda_options(threads = 2)

```

## Introduction

The ultimate goal in this capstone project is to create a shiny app for the predictive modeling of English text. The raw data to create the model was provided by the company [SwiftKey](http://swiftkey.com), which was purchased by Microsoft in 2016. The raw data contains text collected from blog posts, tweets, and news articles, and was downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).
It contains text in English, German, Russian, and Finnish, but we will restrict ourselves to the English language in this project.



```{r read files}

#srcDir <- "data/final/en_US"
srcDir <- "data/sample10"
src <- c(blogs = "blogs", news = "news", twitter = "twitter")

files <- sapply(src, function(s) {
  #    fileName <- paste("en_US", s, "txt", sep = ".")
    fileName <- paste("sample10.en_US", s, "txt", sep = ".")
    paste(srcDir, fileName, sep = "/")
})

fileSizes <- data.frame("File size [MB]" = sapply(src, function(s) {
  file.size(files[s]) / 1024 ^ 2
}), check.names = FALSE)

fileSizes

lines <- sapply(src, function(s) {
    con <- file(files[s], "r")
    lns <- readLines(con, skipNul = TRUE)
    close(con)
    lns
})

counts <- data.frame(
    sapply(lines, function(lns) {
        size <- object.size(lns) / 1024^2
        lineCount <- length(lns)
        c("Data size [MB]" = size, "Lines" = lineCount)
    }) 
)

charCounts <- sapply(lines, function(lns) {
    sapply(lns, nchar, USE.NAMES = FALSE)
})

totalChars <- data.frame(Characters = sapply(charCounts, sum))

counts <- rbind(t(fileSizes), counts, t(totalChars))

counts$total <- rowSums(counts)
kable(counts, digits = 0, format.args = list(big.mark = ' '),
      caption = "Data size and line counts in source files")

```

```{r sample}

samples <- sapply(lines, function(lns) {
    sample(lns, length(lns) / 10, replace = FALSE)
})

sampleText <- rbind(
    data.frame(src = "blogs", txt = samples$blogs),
    data.frame(src = "news", txt = samples$news),
    data.frame(src = "twitter", txt = samples$twitter)
)

sampleText$charCount <- nchar(sampleText$txt)

# get rid of large objects
rm(list = c("lines", "samples"))

```

```{r graphs, message = FALSE}

meltedChar <- melt(charCounts, value.name = "characters") %>%
    filter(characters <= 600)

meltedSample <- melt(sampleText, id.vars = "src", measure.vars = "charCount", value.name = "characters") %>%
    filter(characters <= 600)

ggplot(meltedChar) +
    geom_density(aes(x = characters, col = L1)) +
    geom_density(data = meltedSample, aes(x = characters, col = src), linetype = 2) +
    scale_color_discrete(name = "Source") +
    ggtitle("Density plot of characters per entry")
  
```

```{r tokenize, message=FALSE, eval=TRUE}
tokens_all <- tokens(sampleText$txt,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE)

tokens_nosw <- tokens_remove(tokens_all, stopwords("english"))
```

```{r frequencyplot, eval=TRUE}
token_freq <- function(tokens) {
  dfm <- dfm(tokens)
  freq <- textstat_frequency(dfm)
  freq$cumulFreq <- cumsum(freq$frequency)
  freq
}

freqPlot2 <- function(tk_freq, ntop, xlab = "n-gram", ylab = "Count", title = NULL) {
    tf <- tk_freq[1:ntop, ]
    ggplot(tf, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_col() +
        xlab(xlab) + ylab(ylab) +
        labs(title = title) +
        coord_flip()
}


freqPlot <- function(tokens, ntop, xlab = "n-gram", ylab = "Count", title = NULL) {
    tf <- token_freq(tokens)[1:ntop, ]
    ggplot(tf, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_col() +
        xlab(xlab) + ylab(ylab) +
        labs(title = title) +
        coord_flip()
}
```

```{r unigrams, eval=TRUE}
tk_freq.uni <- token_freq(tokens_all)
uniplot.fr <- freqPlot2(tk_freq.uni, 20, xlab = "Word (unigram)", title = "Including stopwords")

tk_freq.uni.nosw <- token_freq(tokens_nosw)
noswplot.fr <- freqPlot2(tk_freq.uni.nosw, 20, xlab = "Word (unigram)", title = "Excluding stopwords")

grid.arrange(uniplot.fr, noswplot.fr, ncol = 2)

ggplot(tk_freq.uni) +
  geom_line(aes(x = 1:length(cumulFreq), y = cumulFreq)) +
  geom_hline(yintercept = (0.9 * tail(tk_freq.uni$cumulFreq, 1)), col = "blue", linetype = "dashed") +
  ylab("Cumulative Frequency") + xlab("Token index (from most to least occurring)")

totalTokens <- tail(tk_freq.uni$cumulFreq, 1)
totalTokens

minFreqIdx <- min(which(tk_freq.uni$cumulFreq > 0.9 * totalTokens))
minFreq <- tk_freq.uni$frequency[minFreqIdx]

cutIdx <- min(which(tk_freq.uni$frequency < minFreq))
tk_freq.uni[(cutIdx - 5) : (cutIdx + 5), ]
```

```{r nonStopword, eval = TRUE}
minIdx_nonStopword <- min(which(!(tk_freq.uni$feature %in% stopwords("english"))))
```

The first non-stopword is "`r tk_freq.uni$feature[minIdx_nonStopword]`" at position `r minIdx_nonStopword` or `r tk_freq.uni$rank[minIdx_nonStopword]`.

```{r bigrams, eval=FALSE}

tk.bi <- tokens_ngrams(tokens_all, n = 2, concatenator = " ")
biplot <- freqPlot(tk.bi, 20, xlab = "Bigram", title = "Including stopwords")
rm(list = "tk.bi")

tknosw.bi <- tokens_ngrams(tokens_nosw, n = 2, concatenator = " ")
noswplot <- freqPlot(tknosw.bi, 20, xlab = "Bigram", title = "Excluding stopwords")
rm(list = "tknosw.bi")

grid.arrange(biplot, noswplot, ncol = 2)
```

```{r trigrams, eval=FALSE}

tk.tri <- tokens_ngrams(tokens_all, n = 3, concatenator = " ")
triplot <- freqPlot(tk.tri, 20, xlab = "Trigram", title = "Including stopwords")
rm(list = "tk.tri")

tknosw.tri <- tokens_ngrams(tokens_nosw, n = 3, concatenator = " ")
noswplot <- freqPlot(tknosw.tri, 20, xlab = "Trigram", title = "Excluding stopwords")
rm(list = "tknosw.tri")

grid.arrange(triplot, noswplot, ncol = 2)

```
