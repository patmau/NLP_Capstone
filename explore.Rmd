---
title: "Data Science Capstone - Milestone Report"
author: "patmau"
date: "Jan 3, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(quanteda)
library(formattable)
quanteda_options(threads = 2)

```

## Introduction

The ultimate goal in this capstone project is to create a shiny app for the predictive modeling of English text. The raw data to create the model was provided by the company [SwiftKey](http://swiftkey.com), which was purchased by Microsoft in 2016. The raw data contains text collected from blog posts, tweets, and news articles, and was downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

In this report, we describe a first exploratory analysis of the raw data. The data contains text in English, German, Russian, and Finnish, but we will restrict ourselves to the English language in this project. The R code used to produce this report can be found in the appendix.


## Data size

Text is provided in three text files corresponding to the sources of the text (Twitter, blogs, news). Each line in a file corresponds to an entry in the respective source (e.g., a tweet or a blog post).
In a first step, we look at the size of the data. The table below lists file sizes, memory used to store the data in memory, the number of entries in each file and the number of characters.

```{r read files}

# read files and compute file sizes and number of entries/characters

#srcDir <- "data/final/en_US"
srcDir <- "data/sample10"
src <- c(blogs = "blogs", news = "news", twitter = "twitter")

files <- sapply(src, function(s) {
#      fileName <- paste("en_US", s, "txt", sep = ".")
    fileName <- paste("sample10.en_US", s, "txt", sep = ".")
    paste(srcDir, fileName, sep = "/")
})

fileSizes <- data.frame("File size [MB]" = sapply(src, function(s) {
  file.size(files[s]) / 1024 ^ 2
}), check.names = FALSE)

lines <- sapply(src, function(s) {
    con <- file(files[s], "r")
    lns <- readLines(con, skipNul = TRUE)
    close(con)
    lns
})

counts <- data.frame(
    sapply(lines, function(lns) {
        size <- object.size(lns) / 1024^2
        lineCount <- length(lns)
        c("Data size [MB] in memory" = size, "Entries" = lineCount)
    }) 
)

charCounts <- sapply(lines, function(lns) {
    sapply(lns, nchar, USE.NAMES = FALSE)
})

totalChars <- data.frame(Characters = sapply(charCounts, sum))

counts <- rbind(t(fileSizes), counts, t(totalChars))

counts$total <- rowSums(counts)
kable(counts, digits = 0, format.args = list(big.mark = ' '),
      caption = "Data size and number of entries and characters in source files")

```

All three source files contain a similar total amount of data, although the Twitter file contains roughly twice as many entries as the other two files (tweets tend to be shorter than blog posts or news articles, see below).
The total amount of data is `r round(counts$total[2])` MB in memory. 


## Sample the data

The complete raw data set is rather large, and it turns out that the memory requirement for tokenization of the complete data set exceeds what is available on the laptop used for this analysis (8 GB).
Therefore, further analysis will be performed on a subset constructed by randomly sampling 10% of all entries from each source file. 

The figure below compares the distributions of the length of entries (number of characters) in the various sources. Solid lines represent the complete data set, dashed lines represent sampled data.

```{r sample}

# sample raw data
set.seed(2020)
samples <- sapply(lines, function(lns) {
    sample(lns, length(lns) / 10, replace = FALSE)
})

sampleText <- rbind(
    data.frame(src = "blogs", txt = samples$blogs),
    data.frame(src = "news", txt = samples$news),
    data.frame(src = "twitter", txt = samples$twitter)
)

sampleText$charCount <- nchar(sampleText$txt)

# get rid of large objects
rm(list = c("lines", "samples"))

```

```{r graphs, message = FALSE}

# character density plots

# convert to long format for ggplot
meltedChar <- melt(charCounts, value.name = "characters") %>%
    filter(characters <= 600)

meltedSample <- melt(sampleText, id.vars = "src", measure.vars = "charCount", value.name = "characters") %>%
    filter(characters <= 600)

ggplot(meltedChar) +
    geom_density(aes(x = characters, col = L1)) +
    geom_density(data = meltedSample, aes(x = characters, col = src), linetype = 2) +
    scale_color_discrete(name = "Source") +
    ggtitle("Density plot of characters per entry")

```

It is obvious from the plot that Twitter was sourced at a time when tweets were still limited to 140 characters. Blog posts and news articles show a broader distribution of the length of entries. 
The plot also shows that the data sample adequately represents the full set in terms of the length of entries (which can be used as a crude measure of complexity of a text). 

## Words

We use the [quanteda package](http://quanteda.io) to tokenize the text corpus. For an initial analysis, we restrict ourselves to words and remove punctuation characters, symbols, numbers, URLs, etc., from the corpus. Also, all text is converted to lower case.
Tokenization is performed once including [stop words](https://en.wikipedia.org/wiki/Stop_word), and a second time excluding all stop words (the list of stop words used can be found in the appendix).

```{r tokenize, message=FALSE, eval=TRUE}

# tokenize 
tokens_all <- tokens(sampleText$txt,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE)

tokens_nosw <- tokens_remove(tokens_all, stopwords("english"))

```

```{r frequencyplot, eval=TRUE}

# functions for computing/plotting token frequencies

token_freq <- function(tokens) {
  dfm <- dfm(tokens)
  freq <- textstat_frequency(dfm)
  freq$cumulFreq <- cumsum(freq$frequency)
  freq
}

freqPlot <- function(tk_freq, ntop, xlab = "n-gram", ylab = "Count", title = NULL) {
    tf <- tk_freq[1:ntop, ]
    ggplot(tf, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_col() +
        xlab(xlab) + ylab(ylab) +
        labs(title = title) +
        coord_flip()
}

# corpus coverage
corpusCoverage <- function(tk_freq) {
  nTotal <- tail(tk_freq$cumulFreq, 1)
  nUnique <- length(tk_freq$cumulFreq)
  cover <- c(0.5, 0.8, 0.9, 0.95)
  data.frame(cover = cover, percentage = sapply(cover, function(c) {
    idx <- min(which(tk_freq$cumulFreq >= c * nTotal))
    idx / nUnique
  }))
}

# fraction of n-grams that occur only once
onceFraction <- function(tk_freq) {
  sum(tk_freq$frequency == 1) / length(tk_freq$frequency)
}

```

```{r unigrams, eval=TRUE}

# unigrams

tkfreq.uni <- token_freq(tokens_all)
uniplot.fr <- freqPlot(tkfreq.uni, 20, xlab = "Word (unigram)", title = "Including stopwords")

tkfreq.uni.nosw <- token_freq(tokens_nosw)
noswplot.fr <- freqPlot(tkfreq.uni.nosw, 20, xlab = "Word (unigram)", title = "Excluding stopwords")

totalTokens <- tail(tkfreq.uni$cumulFreq, 1)

nTokens <- dim(tkfreq.uni)[1]

stopwordCumulFreq <- sum(tkfreq.uni$frequency[tkfreq.uni$feature %in% stopwords("english")])
nOccurOnce <- sum(tkfreq.uni$frequency == 1)

minIdx_nonStopword <- min(which(!(tkfreq.uni$feature %in% stopwords("english"))))

corpusCov.uni <- corpusCoverage(tkfreq.uni)
onceFrac.uni <- onceFraction(tkfreq.uni)

```

The corpus contains a total of `r format(totalTokens, big.mark=" ")` words, composed from `r format(nTokens, big.mark=" ")` unique words.
From these unique words, `r format(nOccurOnce, big.mark=" ")` (`r percent(onceFrac.uni, digits=1)`) appear only once.
Since they account for only `r percent(nOccurOnce/totalTokens, digits=1)` of all words in the corpus, they will not be used for text prediction, which reduces the size of the predictive model significantly.

The figure below shows the 20 most frequent words in the corpus.

```{r unigramPlot, eval=TRUE}

# plot unigrams
grid.arrange(uniplot.fr, noswplot.fr, ncol = 2, top = "Most Frequent Words")
```

Not surprisingly, the most frequent words are all stop words.
In fact, the `r length(stopwords("english"))` stop words account for `r percent(stopwordCumulFreq / totalTokens, digits = 1)` of all words in the corpus.
The first non-stop word, "`r tkfreq.uni$feature[minIdx_nonStopword]`", is ranked at position `r minIdx_nonStopword`.


## Bi- and Trigrams

```{r bigrams, eval=TRUE}

# Bigrams
tk.bi <- tokens_ngrams(tokens_all, n = 2, concatenator = " ")
tkfreq.bi <- token_freq(tk.bi)
biplot <- freqPlot(tkfreq.bi, 20, xlab = "", title = "Bigrams")

corpusCov.bi <- corpusCoverage(tkfreq.bi)
onceFrac.bi <- onceFraction(tkfreq.bi)
ngrams.bi <- tail(tkfreq.bi$cumulFreq, 1)

rm(list = c("tk.bi", "tkfreq.bi"))
```

```{r trigrams, eval=TRUE}

# Trigrams
tk.tri <- tokens_ngrams(tokens_all, n = 3, concatenator = " ")
tkfreq.tri <- token_freq(tk.tri)
triplot <- freqPlot(tkfreq.tri, 20, xlab = "", title = "Trigrams")

corpusCov.tri <- corpusCoverage(tkfreq.tri)
onceFrac.tri <- onceFraction(tkfreq.tri)
ngrams.tri <- tail(tkfreq.tri$cumulFreq, 1)

rm(list = c("tk.tri", "tkfreq.tri"))
```

In total, there are `r format(ngrams.bi, big.mark=" ")` bigrams and `r format(ngrams.tri, big.mark=" ")` trigrams in the corpus.
The plot below displays the 20 most frequent bi- and trigrams.

```{r bitrigramplot, eval=TRUE}
grid.arrange(biplot, triplot, ncol = 2, top = "2- and 3-grams including stop words")
```

It is interesting to observe that the most frequent bigrams are composed entirely from stop words.
Many of the trigrams, on the other hand, contain one (e.g., "thanks for the") or even two ("looking forward to") non-stop words. 
It remains to be seen if this feature of trigrams can be exploited in the predictive model.

## Corpus coverage by n-grams

We have seen above that in the case of 1-grams, stop words alone cover close to half of all words in the corpus.
On the other hand, `r percent(onceFrac.uni, digits=1)` of 1-grams occur only once.
For bi- and trigrams, the situation is very different. 
The fraction of 2-grams that occur once is `r percent(onceFrac.bi,digits=1)`, and rises to `r percent(onceFrac.tri, digits=1)` with 3-grams.
This also means that the most frequent 2- and 3-grams cover a much smaller fraction of the corpus.

The table below shows which fraction of unique n-grams are needed to cover a given percentage of the corpus.

```{r corpusCoverage, eval=TRUE}
kable_df <- data.frame(percent(corpusCov.uni$cover, digits = 0),
                       percent(corpusCov.uni$percentage, digits = 1),
                       percent(corpusCov.bi$percentage, digits = 1),
                       percent(corpusCov.tri$percentage, digits = 1)
)

kable(kable_df,
   col.names = c("Corpus coverage", "1-grams", "2-grams", "3-grams")
 )

```

## Modeling strategy

The plan is to use an n-gram model to predict the next word based on the previous 1, 2, or 3 words. 
The model will exclude words that occur only once in the corpus (or less than n times, where n will need to be determined).
First, the user is highly unlikely to actually want to type such a rare word (and our odds of predicting it correctly are minuscule), and second and more important, it will significantly reduce the size of our model.
Using a smaller number of 1-grams will also facilitate the creation of n-grams, since memory requirements increase strongly with n, and memory appears to be the limiting factor on the laptop used to create this work.

The analysis presented here so far suggest that 3-grams might be useful to predict the third word from 2-grams in common expressions such as "a lot of", "thank you for", or "as well as".

In this investigation, we have not looked at 4-grams and skip-grams, and we will have to see if they help to improve the model.
The limiting factor in the creation of n-grams (with `n > 2`) and skip-grams appears to be memory, so the we will likely have to split the raw data into manageable chunks and combine the resulting n-grams in a suitable way.

## Appendix

### Stop words

```{r stopwords}
sort(stopwords("english"))
```

### R code

```{r allcode, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
